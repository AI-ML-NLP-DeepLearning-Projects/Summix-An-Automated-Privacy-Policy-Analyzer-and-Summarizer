# -*- coding: utf-8 -*-
"""summarizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RZmmXmGTXe3NG7chwAtuMuPhm3sJdAFW
"""

#importing libraries
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize, sent_tokenize
import os

def _create_dictionary_table(text_string):
   
    #removing stop words
    stop_words = set(stopwords.words("english"))
    
    words = word_tokenize(text_string)
    
    #reducing words to their root form
    stem = PorterStemmer()
    
    #creating dictionary for the word frequency table
    frequency_table = dict()
    for wd in words:
        wd = stem.stem(wd)
        if wd in stop_words:
            continue
        if wd in frequency_table:
            frequency_table[wd] += 1
        else:
            frequency_table[wd] = 1

    return frequency_table


def _calculate_sentence_scores(sentences, frequency_table):   

    #algorithm for scoring a sentence by its words
    sentence_weight = dict()

    for sentence in sentences:
        sentence_wordcount = (len(word_tokenize(sentence)))
        sentence_wordcount_without_stop_words = 0
        for word_weight in frequency_table:
            if word_weight in sentence.lower():
                sentence_wordcount_without_stop_words += 1
                if sentence[:7] in sentence_weight:
                    sentence_weight[sentence[:7]] += frequency_table[word_weight]
                else:
                    sentence_weight[sentence[:7]] = frequency_table[word_weight]

        sentence_weight[sentence[:7]] = sentence_weight[sentence[:7]] / sentence_wordcount_without_stop_words

    return sentence_weight

def _calculate_average_score(sentence_weight):
   
    #calculating the average score for the sentences
    sum_values = 0
    for entry in sentence_weight:
        sum_values += sentence_weight[entry]
    
    average_score = (sum_values / len(sentence_weight))#getting sentence average value from source text

    return average_score

def _get_summary(sentences, sentence_weight, thres):
    
    sentence_counter = 0
    article_summary = ''

    for sentence in sentences:
        if sentence[:7] in sentence_weight and sentence_weight[sentence[:7]] >= (thres):
            article_summary += sentence
            sentence_counter += 1
    print ("summary_sentence : ", sentence_counter)
    return article_summary

def _run_summary(_text):

    frequency_table = _create_dictionary_table(_text)
    #print ('\nfrequency table',frequency_table)
    
    sentences = sent_tokenize(_text)
    #print ('\nsentences', sentences)
    
    sentence_scores = _calculate_sentence_scores(sentences, frequency_table)
    #print ('\nsentence scores',sentence_scores)
    
    threshold = _calculate_average_score(sentence_scores)
    #print ('\nthreshold',threshold)

    _text_summary = _get_summary(sentences, sentence_scores, 1.1 * threshold)
    #print ("\nsummary\n",_text_summary)

    return _text_summary

path = os.getcwd()
privacy_path = path + '/privacy policies'
summary_path = path + '/summaries/'
print (path)

files = []
for r, d, f in os.walk(privacy_path):
    for file in f:
        temp_path = os.path.join(r, file)
        if '.txt' in file and os.stat(temp_path).st_size != 0:
            files.append(temp_path)
    break
    
    
privacy_policies=[]
for file in files:
    f = open(file, 'rt')
    text=f.read()
    privacy_policies.append(text)
    f.close()

i=0
for privacy_policy in privacy_policies:
    print (files[i])
    summary_results = _run_summary(privacy_policy)
    f= open(os.path.join(summary_path , str(i) + '.txt'), "w+")
    f.write(summary_results)
    i+=1

